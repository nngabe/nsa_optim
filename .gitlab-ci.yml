# GitLab CI/CD Pipeline for NSA + Optimizer Ablation Study
#
# This pipeline provides:
# - Secure credential handling via GitLab CI/CD variables
# - Docker image building and tagging
# - Code quality checks (linting, type checking)
# - Comprehensive unit tests for optimizers and models
# - Integration tests on GPU runners
# - Smoke tests for all optimizer/model combinations
# - Experiment execution and artifact collection
#
# Required GitLab CI/CD Variables (Settings > CI/CD > Variables):
# - CI_REGISTRY_USER: Docker registry username (auto-provided by GitLab)
# - CI_REGISTRY_PASSWORD: Docker registry password (auto-provided by GitLab, masked)
# - HF_TOKEN: HuggingFace API token (optional, for model uploads, masked)
# - WANDB_API_KEY: Weights & Biases API key (optional, masked)

stages:
  - lint
  - test
  - build
  - train-smoke
  - train-experiments
  - analyze
  - deploy

# Global variables
variables:
  PIP_CACHE_DIR: "$CI_PROJECT_DIR/.pip-cache"
  TORCH_HOME: "$CI_PROJECT_DIR/.torch-cache"
  HF_HOME: "$CI_PROJECT_DIR/.hf-cache"
  WANDB_MODE: "offline"  # Don't sync during CI, upload artifacts instead
  PYTHONUNBUFFERED: "1"
  # Docker image tags
  DOCKER_IMAGE: "$CI_REGISTRY_IMAGE"
  DOCKER_TAG_SHA: "$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA"
  DOCKER_TAG_BRANCH: "$CI_REGISTRY_IMAGE:$CI_COMMIT_REF_SLUG"
  DOCKER_TAG_LATEST: "$CI_REGISTRY_IMAGE:latest"

# Cache configuration for faster builds
.pip-cache: &pip-cache
  cache:
    key: pip-${CI_COMMIT_REF_SLUG}
    paths:
      - .pip-cache/
      - .torch-cache/
      - .hf-cache/
    policy: pull-push

# Default image for non-GPU jobs
default:
  image: python:3.12-slim
  before_script:
    - pip install --upgrade pip

# ==============================================================================
# STAGE: Lint - Code quality checks
# ==============================================================================

lint:ruff:
  stage: lint
  <<: *pip-cache
  script:
    - pip install ruff
    - ruff check . --output-format=gitlab || true
  allow_failure: true
  artifacts:
    reports:
      codequality: gl-code-quality-report.json
    when: always

lint:black:
  stage: lint
  <<: *pip-cache
  script:
    - pip install black
    - black --check --diff .
  allow_failure: true

# ==============================================================================
# STAGE: Test - Unit and integration tests
# ==============================================================================

test:unit-cpu:
  stage: test
  <<: *pip-cache
  script:
    - pip install pytest pytest-cov torch numpy
    - pip install -r requirements.txt || echo "Some requirements may fail on CPU"
    - pytest tests/unit/ -v -m "not gpu" --cov=. --cov-report=xml --cov-report=term
  coverage: '/TOTAL.*\s+(\d+%)/'
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: coverage.xml
    paths:
      - coverage.xml
    expire_in: 1 week
  allow_failure: true

test:config:
  stage: test
  <<: *pip-cache
  script:
    - pip install torch
    - python -c "from config import get_experiment_grid; exps = get_experiment_grid(); print(f'Generated {len(exps)} experiments')"
    - python -c "from config import parse_model_size; print('100M params:', parse_model_size('100M'))"
    - python -c "from config import OptimizerType; print('Optimizer types:', [o.value for o in OptimizerType])"

test:imports:
  stage: test
  <<: *pip-cache
  script:
    - pip install torch transformers
    - python -c "import torch; print(f'PyTorch version: {torch.__version__}')"
    - python -c "from config import *; print('Config module loaded')"
    - python -c "from optimizers import create_optimizer; print('Optimizers module loaded')"
    - python -c "from data import create_dataloader; print('Data module loaded')"

# ==============================================================================
# STAGE: Build - Create and tag Docker images
# ==============================================================================

build:docker:
  stage: build
  image: docker:24.0
  services:
    - docker:24.0-dind
  variables:
    DOCKER_TLS_CERTDIR: "/certs"
    DOCKER_DRIVER: overlay2
  before_script:
    # Login to GitLab Container Registry using secure CI variables
    # CI_REGISTRY_USER and CI_REGISTRY_PASSWORD are automatically provided by GitLab
    - echo "Logging in to $CI_REGISTRY"
    - echo "$CI_REGISTRY_PASSWORD" | docker login -u "$CI_REGISTRY_USER" --password-stdin "$CI_REGISTRY"
  script:
    # Build Docker image
    - echo "Building Docker image from docker/Dockerfile"
    - docker build -f docker/Dockerfile -t "$DOCKER_TAG_SHA" .

    # Tag with commit SHA
    - docker push "$DOCKER_TAG_SHA"
    - echo "✓ Pushed $DOCKER_TAG_SHA"

    # Tag with branch name
    - docker tag "$DOCKER_TAG_SHA" "$DOCKER_TAG_BRANCH"
    - docker push "$DOCKER_TAG_BRANCH"
    - echo "✓ Pushed $DOCKER_TAG_BRANCH"

    # Tag as latest if on main branch
    - |
      if [ "$CI_COMMIT_BRANCH" == "main" ] || [ "$CI_COMMIT_BRANCH" == "master" ]; then
        docker tag "$DOCKER_TAG_SHA" "$DOCKER_TAG_LATEST"
        docker push "$DOCKER_TAG_LATEST"
        echo "✓ Pushed $DOCKER_TAG_LATEST"
      fi
  after_script:
    - docker logout "$CI_REGISTRY"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_COMMIT_BRANCH == "develop"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - when: manual
  allow_failure: false

# Test the built Docker image
test:docker-image:
  stage: test
  image: $DOCKER_TAG_SHA
  needs:
    - build:docker
  script:
    - python --version
    - pip list | grep -E "torch|transformers|flash|mamba|liger|emerging"
    - python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
    - python -c "from config import *; from models import *; from optimizers import *; print('All modules loaded successfully')"
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"

# ==============================================================================
# STAGE: Train Smoke Tests - Quick validation with GPU
# ==============================================================================

.train-template: &train-template
  image: $DOCKER_TAG_SHA
  tags:
    - gpu
    - cuda
  needs:
    - build:docker
  timeout: 2h
  artifacts:
    paths:
      - outputs/
      - wandb/
    expire_in: 1 week
  retry:
    max: 1
    when:
      - runner_system_failure
      - stuck_or_timeout_failure

# Run comprehensive smoke tests using the smoke_test.sh script
train:smoke-tests-all:
  stage: train-smoke
  <<: *train-template
  timeout: 4h
  script:
    - bash scripts/smoke_test.sh
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
    - when: manual
  allow_failure: true

# Individual smoke tests for critical configurations

train:smoke-adamw8bit-a-dense:
  stage: train-smoke
  <<: *train-template
  script:
    - python train.py
        --model_size 100M
        --attn_type dense
        --optimizer_type adamw8bit
        --context_length 512
        --num_train_steps 10
        --batch_size 2
        --gradient_accumulation_steps 2
        --dtype bfloat16
        --log_interval 5
        --output_dir outputs/smoke-adamw8bit-a-dense
        --run_name smoke-adamw8bit-a-dense
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"

train:smoke-adamw8bit-mdma:
  stage: train-smoke
  <<: *train-template
  script:
    - python train.py
        --model_size 100M
        --block_pattern MDMA
        --block_repeats 2
        --optimizer_type adamw8bit
        --context_length 512
        --num_train_steps 10
        --batch_size 2
        --gradient_accumulation_steps 2
        --dtype bfloat16
        --log_interval 5
        --output_dir outputs/smoke-adamw8bit-mdma
        --run_name smoke-adamw8bit-mdma
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"

train:smoke-soap8bit-mdma:
  stage: train-smoke
  <<: *train-template
  script:
    - python train.py
        --model_size 100M
        --block_pattern MDMA
        --block_repeats 2
        --optimizer_type soap8bit
        --precondition_frequency 5
        --context_length 512
        --num_train_steps 10
        --batch_size 2
        --gradient_accumulation_steps 2
        --dtype bfloat16
        --log_interval 5
        --output_dir outputs/smoke-soap8bit-mdma
        --run_name smoke-soap8bit-mdma
  rules:
    - if: $CI_PIPELINE_SOURCE == "merge_request_event"
    - if: $CI_COMMIT_BRANCH == "main"
  allow_failure: true  # SOAP may require specific dependencies

# ==============================================================================
# STAGE: Train Experiments - Full ablation study
# ==============================================================================

.experiment-template: &experiment-template
  stage: train-experiments
  <<: *train-template
  timeout: 24h
  artifacts:
    paths:
      - outputs/
      - wandb/
    expire_in: 1 month
  rules:
    - if: $RUN_EXPERIMENTS == "true"
    - if: $CI_COMMIT_TAG
    - when: manual
      allow_failure: true

# Experiment: AdamW8bit with different architectures
train:exp-adamw8bit-architectures:
  <<: *experiment-template
  parallel:
    matrix:
      - BLOCK_PATTERN: ["A", "D", "M", "MDMA"]
        ATTENTION: ["dense", "native_sparse_attention"]
  script:
    - |
      # Set attention based on block pattern
      if [ "$BLOCK_PATTERN" == "A" ]; then
        ATTN_FLAG="--attn_type $ATTENTION"
        PATTERN_FLAG=""
      else
        ATTN_FLAG=""
        PATTERN_FLAG="--block_pattern $BLOCK_PATTERN --block_repeats 4"
      fi
    - python train.py
        --model_size 0.6B
        $ATTN_FLAG
        $PATTERN_FLAG
        --optimizer_type adamw8bit
        --context_length 8192
        --num_train_steps 5000
        --batch_size 4
        --gradient_accumulation_steps 4
        --dtype bfloat16
        --gradient_checkpointing
        --output_dir outputs/exp-adamw8bit-${BLOCK_PATTERN}-${ATTENTION}
        --run_name exp-adamw8bit-${BLOCK_PATTERN}-${ATTENTION}

# Experiment: Low-bit optimizers (SOAP 4-bit and 8-bit)
train:exp-lowbit-optimizers:
  <<: *experiment-template
  parallel:
    matrix:
      - OPTIMIZER: ["soap8bit", "soap4bit"]
  script:
    - python train.py
        --model_size 0.6B
        --block_pattern MDMA
        --block_repeats 4
        --optimizer_type $OPTIMIZER
        --precondition_frequency 10
        --context_length 8192
        --num_train_steps 5000
        --batch_size 4
        --gradient_accumulation_steps 4
        --dtype bfloat16
        --gradient_checkpointing
        --output_dir outputs/exp-${OPTIMIZER}-mdma
        --run_name exp-${OPTIMIZER}-mdma

# ==============================================================================
# STAGE: Analyze - Aggregate and compare results
# ==============================================================================

analyze:results:
  stage: analyze
  image: python:3.12
  script:
    - pip install numpy pandas matplotlib seaborn
    - |
      python -c "
      import os
      import json
      import glob

      print('Collecting experiment results...')
      results = []
      for output_dir in glob.glob('outputs/*/'):
          config_file = os.path.join(output_dir, 'config.json')
          if os.path.exists(config_file):
              with open(config_file) as f:
                  config = json.load(f)
                  results.append({
                      'experiment': os.path.basename(output_dir.rstrip('/')),
                      'optimizer': config.get('optimizer_type'),
                      'model_type': config.get('model_type'),
                      'loss': config.get('final_loss', 'N/A'),
                  })

      print(f'Found {len(results)} experiments')
      for r in results:
          print(f\"  {r['experiment']}: optimizer={r['optimizer']}, loss={r['loss']}\")

      with open('experiment_summary.json', 'w') as f:
          json.dump(results, f, indent=2)
      "
  artifacts:
    paths:
      - experiment_summary.json
    expire_in: 3 months
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
    - if: $RUN_EXPERIMENTS == "true"
  allow_failure: true

# ==============================================================================
# STAGE: Deploy - Upload models and results
# ==============================================================================

deploy:huggingface:
  stage: deploy
  image: python:3.12
  before_script:
    - pip install huggingface_hub
  script:
    - |
      # Only run if HF_TOKEN is set (use GitLab CI/CD masked variable)
      if [ -z "$HF_TOKEN" ]; then
        echo "⚠ HF_TOKEN not set, skipping HuggingFace upload"
        exit 0
      fi
    - |
      python -c "
      from huggingface_hub import HfApi
      import os
      import json
      import glob

      # Use HF_TOKEN from masked GitLab CI variable
      api = HfApi(token=os.environ.get('HF_TOKEN'))

      # Find best checkpoint by loss
      best_loss = float('inf')
      best_path = None

      for exp_dir in glob.glob('outputs/*/'):
          config_path = os.path.join(exp_dir, 'config.json')
          if os.path.exists(config_path):
              with open(config_path) as f:
                  config = json.load(f)
                  loss = config.get('final_loss', float('inf'))
                  if loss < best_loss:
                      best_loss = loss
                      best_path = exp_dir

      if best_path and os.environ.get('HF_REPO_ID'):
          print(f'Uploading best model from {best_path} (loss={best_loss:.4f})')
          api.upload_folder(
              folder_path=best_path,
              repo_id=os.environ['HF_REPO_ID'],
              repo_type='model',
          )
          print('✓ Model uploaded successfully')
      else:
          print('No model to upload or HF_REPO_ID not set')
      "
  rules:
    - if: $CI_COMMIT_TAG
    - if: $DEPLOY_MODELS == "true"
  allow_failure: true

# Save container image info
deploy:container-manifest:
  stage: deploy
  image: alpine
  script:
    - |
      cat > container-manifest.json <<EOF
      {
        "image": "$DOCKER_TAG_SHA",
        "tags": [
          "$DOCKER_TAG_BRANCH",
          "$DOCKER_TAG_SHA"
        ],
        "commit": "$CI_COMMIT_SHA",
        "branch": "$CI_COMMIT_REF_NAME",
        "built_at": "$(date -Iseconds)"
      }
      EOF
    - cat container-manifest.json
  artifacts:
    paths:
      - container-manifest.json
    expire_in: 1 year
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
  needs:
    - build:docker

# ==============================================================================
# Scheduled Pipelines
# ==============================================================================

# Nightly smoke tests
nightly:smoke:
  stage: train-smoke
  <<: *train-template
  script:
    - bash scripts/smoke_test.sh
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $SCHEDULE_TYPE == "nightly"
  allow_failure: true

# Weekly container rebuild (to pick up base image security updates)
weekly:rebuild-container:
  stage: build
  extends: build:docker
  rules:
    - if: $CI_PIPELINE_SOURCE == "schedule" && $SCHEDULE_TYPE == "weekly"
